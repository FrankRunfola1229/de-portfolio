[
  {
    "title": "Lab 01 — Create a DataFrame + inspect schema",
    "goal": "Build a tiny DataFrame, inspect schema, then add a derived column.",
    "runPath": "Run in Databricks notebook. No input files needed.",
    "expected": "You see 3 rows, a schema with customer_id/name/amount, and a new amount_taxed column.",
    "steps": [
      "Create a small dataset in code",
      "Create a DataFrame",
      "printSchema() + show()",
      "Add a column and re-check"
    ],
    "code": "from pyspark.sql import functions as F\n\n# Databricks already provides `spark`.\n# This lab uses in-memory data only.\n\nrows = [\n  (1, \"alice\", 120.50),\n  (2, \"bob\",  75.00),\n  (3, \"cara\", 10.00)\n]\n\ndf = spark.createDataFrame(rows, [\"customer_id\", \"name\", \"amount\"])\n\nprint(\"rows:\", df.count())\ndf.printSchema()\ndf.show(truncate=False)\n\n# Simple derived column (tax / markup)\ndf2 = df.withColumn(\"amount_taxed\", F.round(F.col(\"amount\") * F.lit(1.08), 2))\ndf2.show(truncate=False)\n"
  },
  {
    "title": "Lab 02 — Read CSV with schema + write Delta",
    "goal": "Read CSV safely (explicit schema), do basic cleanup, then write Delta.",
    "runPath": "Input: /mnt/bronze/in/customers.csv → Output: /mnt/silver/customers",
    "expected": "A Delta table exists at /mnt/silver/customers and df.count() matches your input (minus null keys).",
    "steps": [
      "Define schema",
      "Read CSV",
      "Trim + drop null keys",
      "Write Delta"
    ],
    "code": "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\nfrom pyspark.sql import functions as F\n\nschema = StructType([\n  StructField(\"customer_id\", IntegerType(), True),\n  StructField(\"name\", StringType(), True),\n  StructField(\"amount\", DoubleType(), True)\n])\n\nbronze = (\n  spark.read\n    .option(\"header\", \"true\")\n    .schema(schema)\n    .csv(\"/mnt/bronze/in/customers.csv\")\n)\n\nsilver = (\n  bronze\n    .withColumn(\"name\", F.trim(F.col(\"name\")))\n    .dropna(subset=[\"customer_id\"])  # keys must exist\n)\n\nprint(\"bronze:\", bronze.count(), \"silver:\", silver.count())\n\n(silver.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(\"/mnt/silver/customers\"))\n"
  },
  {
    "title": "Lab 03 — Flatten JSON + explode arrays (API-style)",
    "goal": "Turn nested API JSON into tabular rows (1 row per order item).",
    "runPath": "Input: /mnt/bronze/api/orders.json → Output: /mnt/silver/order_items",
    "expected": "You get columns: order_id, customer_id, sku, qty, price and rows per item.",
    "steps": [
      "Read multiLine JSON",
      "Explode orders array",
      "Explode items array",
      "Select nested fields",
      "Write Delta"
    ],
    "code": "from pyspark.sql import functions as F\n\nraw = (spark.read\n  .option(\"multiLine\", \"true\")\n  .json(\"/mnt/bronze/api/orders.json\"))\n\n# raw.orders = [ { id, customerId, items: [ {sku, qty, price}, ... ] }, ... ]\norders = raw.select(F.explode_outer(\"orders\").alias(\"o\")).select(\"o.*\")\n\nitems = (\n  orders\n    .select(\n      F.col(\"id\").alias(\"order_id\"),\n      F.col(\"customerId\").alias(\"customer_id\"),\n      F.explode_outer(\"items\").alias(\"it\")\n    )\n    .select(\n      \"order_id\",\n      \"customer_id\",\n      F.col(\"it.sku\").alias(\"sku\"),\n      F.col(\"it.qty\").alias(\"qty\"),\n      F.col(\"it.price\").alias(\"price\")\n    )\n)\n\nitems.show(10, truncate=False)\n\n(items.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(\"/mnt/silver/order_items\"))\n"
  },
  {
    "title": "Lab 04 — Deduplicate keeping latest record",
    "goal": "Keep the latest record per business key using a window function.",
    "runPath": "Input: /mnt/bronze/customer_updates → Output: (in-memory df `latest`)",
    "expected": "One row per customer_id, newest updated_at retained.",
    "steps": [
      "Partition by customer_id",
      "Order by updated_at desc",
      "Keep rn = 1"
    ],
    "code": "from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nupdates = spark.read.format(\"delta\").load(\"/mnt/bronze/customer_updates\")\n\nw = Window.partitionBy(\"customer_id\").orderBy(F.col(\"updated_at\").desc())\n\nlatest = (\n  updates\n    .withColumn(\"rn\", F.row_number().over(w))\n    .filter(F.col(\"rn\") == 1)\n    .drop(\"rn\")\n)\n\nlatest.show(10, truncate=False)\n\n# Quick check: duplicates should be gone\n(latest.groupBy(\"customer_id\").count()\n  .filter(F.col(\"count\") > 1)\n  .show())\n"
  },
  {
    "title": "Lab 05 — Join + Gold KPI rollup",
    "goal": "Join facts to a dimension and produce a simple daily KPI table.",
    "runPath": "Inputs: /mnt/silver/orders + /mnt/silver/customers → Output: /mnt/gold/daily_kpis",
    "expected": "A daily_kpis Delta table with order_date, orders, unique_customers, revenue.",
    "steps": [
      "Load Silver tables",
      "Left join dims",
      "Group to daily KPIs",
      "Write Gold output"
    ],
    "code": "from pyspark.sql import functions as F\n\norders = spark.read.format(\"delta\").load(\"/mnt/silver/orders\")\ncustomers = spark.read.format(\"delta\").load(\"/mnt/silver/customers\")\n\n# Keep all orders even if customer lookup is missing\njoined = orders.join(customers.select(\"customer_id\", \"name\"), \"customer_id\", \"left\")\n\nkpis = (\n  joined\n    .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n    .groupBy(\"order_date\")\n    .agg(\n      F.count(\"*\").alias(\"orders\"),\n      F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n      F.sum(\"order_total\").alias(\"revenue\")\n    )\n)\n\nkpis.orderBy(\"order_date\").show(10, truncate=False)\n\n(kpis.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(\"/mnt/gold/daily_kpis\"))\n"
  },
  {
    "title": "Lab 06 — Quarantine bad rows (data quality)",
    "goal": "Split bad rows into quarantine instead of silently dropping them.",
    "runPath": "Input: /mnt/bronze/events → Output: /mnt/quarantine/bad_events + /mnt/silver/events_clean",
    "expected": "You get two Delta outputs and counts printed for good vs bad.",
    "steps": [
      "Define simple quality rules",
      "Split good vs bad",
      "Write quarantine + write clean Silver"
    ],
    "code": "from pyspark.sql import functions as F\n\ndf = spark.read.format(\"delta\").load(\"/mnt/bronze/events\")\n\nbad = df.filter(\n  F.col(\"customer_id\").isNull() |\n  F.col(\"event_ts\").isNull() |\n  (F.col(\"amount\") < 0)\n)\n\ngood = df.subtract(bad)\n\nbad.write.format(\"delta\").mode(\"append\").save(\"/mnt/quarantine/bad_events\")\ngood.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/silver/events_clean\")\n\nprint(\"good:\", good.count(), \"bad:\", bad.count())\n"
  },
  {
    "title": "Lab 07 — Delta MERGE (upsert) into a dimension",
    "goal": "Upsert updates into a Delta dimension table (real-world requirement).",
    "runPath": "Input: /mnt/silver/customer_updates → Target: /mnt/silver/dim_customer",
    "expected": "Existing customer rows update, new customers insert (no duplicates by key).",
    "steps": [
      "Load updates source",
      "MERGE into target",
      "Insert new, update existing"
    ],
    "code": "from delta.tables import DeltaTable\n\nupdates = spark.read.format(\"delta\").load(\"/mnt/silver/customer_updates\")\ntarget_path = \"/mnt/silver/dim_customer\"\n\ntarget = DeltaTable.forPath(spark, target_path)\n\n(target.alias(\"t\")\n  .merge(updates.alias(\"s\"), \"t.customer_id = s.customer_id\")\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .execute())\n"
  }
]
